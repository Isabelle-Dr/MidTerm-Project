{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Prediction\n",
    "\n",
    "The purpose of this notebook is to provide prediction for flight delays based on provided flight information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INPUT**: flights_test.csv <br>\n",
    "**INPUT**: scaler.pkl, pca.pkl and model_reg.pkl <br>\n",
    "**OUTPUT**: regression_submission.csv <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "import datetime\n",
    "import calendar\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Data\n",
    "Create 2 data frames <br>\n",
    "1. To be transformed and used in model to predict \n",
    "2. As the output file framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df = pd.read_csv(\"flights_test_week.csv\")\n",
    "df.drop(columns = 'Unnamed: 0', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = df.copy()\n",
    "df_submission = df_submission[[\"fl_date\",\"mkt_carrier\",\"mkt_carrier_fl_num\",\"origin\",\"dest\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepping the Data\n",
    "on the df_model we will perform all the same steps we performed on our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month(date):\n",
    "    \"\"\"Returns the month the flight occurs\n",
    "    INPUT date in form YYYY-MM-DD\n",
    "    RETURNS month\"\"\"\n",
    "    DATE = datetime.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    return DATE.month\n",
    "\n",
    "def get_day_of_week(date):\n",
    "    \"\"\"Returns the day of the week\n",
    "    INPUT date in the form YYYY-MM-DD\n",
    "    RETURNS number of day of the week:\n",
    "            where 0 = Monday and 6 = Sunday\"\"\"\n",
    "    DATE = datetime.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    return DATE.weekday()\n",
    "\n",
    "calendar.setfirstweekday(6)\n",
    "\n",
    "def get_week_of_month(date):\n",
    "    date = str(date).split(\"-\")\n",
    "    year, month, day = int(date[0]), int(date[1]), int(date[2])\n",
    "    x = np.array(calendar.monthcalendar(year, month))\n",
    "    week_of_month = np.where(x==day)[0][0] + 1\n",
    "    return(week_of_month)\n",
    "\n",
    "def check_codeshare(branded_codeshare):\n",
    "    words = branded_codeshare.split(\"_\")\n",
    "    if len(words) == 2:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def check_time_block(time):\n",
    "    \"\"\"Determine the hour of the day\n",
    "    INPUT time HHMM\n",
    "    RETURNS hour 0-23 where 0 == 12am and 23 == 11pm\"\"\"\n",
    "    if type(time) == 'numpy.ndarray':\n",
    "        time = time.astype(int)\n",
    "    time = int(time)\n",
    "    \n",
    "    t = time // 100\n",
    "    if t == 0:\n",
    "        return 0\n",
    "    return t-1\n",
    "\n",
    "def define_haul_length(distance):\n",
    "    \"\"\"Determine the haul length of a given distance\n",
    "    INPUT distance (in miles)\n",
    "    RETURNS {0: short haul, 1: medium haul, 2: long haul}\"\"\"\n",
    "    \n",
    "    if distance < 700:\n",
    "        return 0 \n",
    "    elif distance < 3000:\n",
    "        return 1\n",
    "    else: \n",
    "        return 2\n",
    "    \n",
    "def mkt_op_combo(mkt_unique, op_unique):\n",
    "    \"\"\"Output a string for the unique combination of the mkt_unique and op_unique\n",
    "        INPUT strings mkt_unique and op_unique\n",
    "        RETURNS string of unique combo\"\"\"\n",
    "    combo = mkt_unique+op_unique\n",
    "    return combo\n",
    "\n",
    "def flight_type(fl_num):\n",
    "    \"\"\"Takes in a fl_num and returns the type of flight\n",
    "        INPUT fl_num\n",
    "        RETURNs flight_type \n",
    "                2: premium, 1 : regular, 0 : regional, ferry, codeshare\"\"\"\n",
    "    fl_num = str(fl_num)\n",
    "    if len(fl_num) < 3:\n",
    "        return 2 #preimum\n",
    "    if len(fl_num) == 3:\n",
    "        return 1 #reg\n",
    "    else: \n",
    "        return 0 #regional affiliate, ferry, codeshare, etc.\n",
    "\n",
    "def get_hist_m_dep_delay(dep_hour):\n",
    "    \"\"\"returns the mean dep_delay for the hour based on 2018-2019 data\n",
    "        INPUT departure hour\n",
    "        OUTPUT historical_mean_dep_delay\"\"\"\n",
    "    historical_delays = {0: 6.711716493969764, 1: 9.675249362096961, 2: 10.619793205317578, 3: 6.630692167577413, 4: -0.4136475056746801,\n",
    "                         5: 0.11429594186434255, 6: 1.6161646966248562, 7: 2.9157571077032296, 8: 4.135963064069914, 9: 5.26489329589765,\n",
    "                         10: 6.13392546097791, 11: 7.0181790875984875, 12: 8.131680643107183, 13: 9.510919588229374, 14: 10.304524506138106,\n",
    "                         15: 11.336050018124949, 16: 12.382205588783819, 17: 13.536146413960237, 18: 13.756275088751552, 19: 13.289175306396258,\n",
    "                         20: 12.634817154337778, 21: 10.544540416974117, 22: 7.987786063255336}\n",
    "    hour = int(dep_hour)\n",
    "    return historical_delays[hour]\n",
    "\n",
    "def get_hist_m_arr_delay(dep_hour):\n",
    "    \"\"\"returns the mean arr_delay for the hour based on 2018-2019 data\n",
    "        INPUT departure hour\n",
    "        OUTPUT historical_mean_arr_delay\"\"\"\n",
    "    historical_delays = {0: -1.6642602344148123, 1: 3.440501043841336, 2: 4.343870014771049, 3: 1.3802367941712204, 4: -5.484170909895949,\n",
    "                         5: -4.606989162142505, 6: -2.907391092361358, 7: -1.8630207558845115, 8: -1.085556598557783, 9: 0.08580366114366288,\n",
    "                         10: 0.5798795108102116, 11: 2.115288435274042, 12: 3.1173279785990315, 13: 4.805705640109731, 14: 5.449820220021156,\n",
    "                         15: 6.739257741261025, 16: 7.6573414085094385, 17: 8.877695016385436, 18: 8.378205975005564, 19: 7.965689735642226,\n",
    "                         20: 6.8922934459978995, 21: 4.922862682118726, 22: 0.5166495242993058}\n",
    "    \n",
    "    hour = int(dep_hour)\n",
    "    return historical_delays[hour]\n",
    "\n",
    "def get_hist_med_dep_delay(dep_hour):\n",
    "    \"\"\"returns the median dep_delay for the hour based on 2018-2019 data\n",
    "        INPUT departure hour\n",
    "        OUTPUT historical_median_dep_delay\"\"\"\n",
    "    historical_delays = {0: -2.0, 1: -2.0, 2: -3.0, 3: -4.0, 4: -4.0, 5: -4.0, 6: -4.0, 7: -3.0, 8: -3.0, 9: -3.0, 10: -2.0, 11: -2.0,\n",
    "                         12: -2.0, 13: -1.0, 14: -1.0, 15: -1.0, 16: -1.0, 17: -1.0, 18: -1.0, 19: -1.0, 20: -1.0, 21: -1.0, 22: -2.0}\n",
    "    hour = int(dep_hour)\n",
    "    return historical_delays[hour]\n",
    "\n",
    "def get_hist_med_arr_delay(dep_hour):\n",
    "    \"\"\"returns the median arr_delay for the hour based on 2018-2019 data\n",
    "        INPUT departure hour\n",
    "        OUTPUT historical_med_arr_delay\"\"\"\n",
    "    historical_delays = {0: -9.0, 1: -6.0, 2: -6.0, 3: -7.0, 4: -9.0, 5: -9.0, 6: -8.0, 7: -8.0, 8: -7.0, 9: -7.0, 10: -7.0, 11: -6.0,\n",
    "                         12: -6.0, 13: -5.0, 14: -5.0, 15: -4.0, 16: -4.0, 17: -3.0, 18: -3.0, 19: -3.0, 20: -4.0, 21: -4.0, 22: -7.0}\n",
    "    hour = int(dep_hour)\n",
    "    return historical_delays[hour]\n",
    "\n",
    "def get_delay_type(carrier_delay, weather_delay, nas_delay, security_delay, late_aircraft_delay):\n",
    "    \"\"\"Takes in all potential delay causes and returns the primary cause\n",
    "    in case of 2 identical delays the first is returned\n",
    "    INPUT carrier_delay, weather_delay, nas_delay, security_delay, late_aircraft_delay\n",
    "    RETURNS {0 : no delay cause noted,\n",
    "            1 : carrier_delay,\n",
    "            2 : weather_delay,\n",
    "            3 : NAS_delay,\n",
    "            4 : security_delay,\n",
    "            5 : late_aircraft_delay}\"\"\"\n",
    "    \n",
    "    if carrier_delay == weather_delay == nas_delay == security_delay == late_aircraft_delay == 0:\n",
    "        return 0\n",
    "    \n",
    "    delays = {}\n",
    "    delays[1] = carrier_delay\n",
    "    delays[2] = weather_delay\n",
    "    delays[3] = nas_delay\n",
    "    delays[4] = security_delay\n",
    "    delays[5] = late_aircraft_delay\n",
    "    \n",
    "    return max(delays, key=delays.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.drop(columns = ['mkt_carrier', 'tail_num', 'op_carrier_fl_num', \n",
    "                         'origin_airport_id', 'dest_airport_id', 'dup', \n",
    "                         'flights', 'origin_city_name','dest_city_name'], inplace = True)\n",
    "\n",
    "#fl_month\n",
    "df_model.loc[:,'fl_month'] = df_model['fl_date'].apply(get_month)\n",
    "#fl_day_of_week\n",
    "df_model.loc[:,'fl_day_of_week'] = df_model['fl_date'].apply(get_day_of_week)\n",
    "#week of month\n",
    "df_model.loc[:,'fl_week_of_month'] = df_model['fl_date'].apply(get_week_of_month)\n",
    "#drop_fl_date\n",
    "df_model.drop(columns = 'fl_date', inplace=True)\n",
    "\n",
    "#get mkt_op_combo\n",
    "df_model.loc[:,\"mkt_op_combo\"] = df_model.apply(lambda x: mkt_op_combo(x.mkt_unique_carrier, \n",
    "                                                                       x.op_unique_carrier), \n",
    "                                                axis=1)\n",
    "df_model.drop(columns = ['mkt_unique_carrier', 'op_unique_carrier'], inplace = True)\n",
    "\n",
    "df_model.loc[:,\"branded_code_share\"] = df_model[\"branded_code_share\"].apply(check_codeshare)\n",
    "\n",
    "#get fl_type\n",
    "df_model.loc[:,\"fl_type\"] = df_model[\"mkt_carrier_fl_num\"].apply(flight_type)\n",
    "\n",
    "df_model.drop(columns = 'mkt_carrier_fl_num', inplace=True)\n",
    "\n",
    "df_model.loc[:,'crs_dep_time'] = df_model['crs_dep_time'].apply(check_time_block)\n",
    "df_model.loc[:,'m_hist_dep_delay'] = df_model['crs_dep_time'].apply(get_hist_m_dep_delay)\n",
    "df_model.loc[:,'med_hist_dep_delay'] = df_model['crs_dep_time'].apply(get_hist_med_dep_delay)\n",
    "df_model.loc[:,'m_hist_arr_delay'] = df_model['crs_dep_time'].apply(get_hist_m_arr_delay)\n",
    "df_model.loc[:,'med_hist_arr_delay'] = df_model['crs_dep_time'].apply(get_hist_med_arr_delay)\n",
    "\n",
    "df_model.loc[:,'crs_arr_time'] = df_model['crs_arr_time'].apply(check_time_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "air_time was carried in training despite being a prediction factor as such due to time limitations we have replaced that column with the average from that column to allow completion of the assignment, if possible we would have returned to the error to correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.insert (6, \"air_time\", 106.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in encoders \n",
    "loaded_origin = pickle.load(open('Revamp/origin_encoder.pkl', 'rb'))\n",
    "loaded_dest = pickle.load(open('Revamp/dest_encoder.pkl', 'rb'))\n",
    "loaded_mktopcombo = pickle.load(open('Revamp/mkt_op_combo.pickle', 'rb'))\n",
    "df_model.loc[:,\"origin\"] = loaded_origin.transform(df_model.origin)\n",
    "df_model.loc[:,\"dest\"] = loaded_dest.transform(df_model.dest)\n",
    "df_model.loc[:,\"mkt_op_combo\"] = loaded_mktopcombo.transform(df_model.mkt_op_combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['branded_code_share', 'origin', 'dest', 'crs_dep_time', 'crs_arr_time',\n",
       "       'crs_elapsed_time', 'air_time', 'distance', 'fl_month',\n",
       "       'fl_day_of_week', 'fl_week_of_month', 'mkt_op_combo', 'fl_type',\n",
       "       'm_hist_dep_delay', 'med_hist_dep_delay', 'm_hist_arr_delay',\n",
       "       'med_hist_arr_delay'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the Data\n",
    "we must scale our model using the same scaler used on our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in encoder\n",
    "loaded_scale = pickle.load(open('Revamp/scaler.pkl', 'rb'))\n",
    "\n",
    "X_scaled = loaded_scale.transform(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.77880755,  0.72158282,  1.38409982, ...,  0.92740247,\n",
       "         1.39219834,  1.42838864],\n",
       "       [-0.77880755,  0.72158282,  1.38409982, ...,  0.04916336,\n",
       "        -0.4730043 , -0.61259016],\n",
       "       [-0.77880755,  0.72158282,  1.4434826 , ...,  0.92740247,\n",
       "         1.18719563,  1.42838864],\n",
       "       ...,\n",
       "       [ 1.28401426, -1.59489377,  1.84926496, ...,  0.92740247,\n",
       "         1.11788436,  0.91814394],\n",
       "       [ 1.28401426,  1.8501227 , -1.59493654, ...,  0.92740247,\n",
       "         1.39219834,  1.42838864],\n",
       "       [ 1.28401426,  1.08786331,  0.21623839, ..., -1.70731486,\n",
       "        -1.63892094, -1.63307956]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction\n",
    "we must reduce our features using the same pca() as determined on our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_values = X_scaled.copy()\n",
    "#load in the pca\n",
    "loaded_pca = pickle.load(open('pca.pkl', 'rb'))\n",
    "component = loaded_pca.transform(X_scaled)\n",
    "w_transpose = np.transpose(loaded_pca.components_)\n",
    "reduced_feat = np.matmul(data_values, w_transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = pd.DataFrame(data = X_scaled,\n",
    "              columns = ['branded_code_share', 'origin', 'dest', 'crs_dep_time', \n",
    "                         'crs_arr_time','crs_elapsed_time', 'air_time', 'distance', \n",
    "                         'fl_month', 'fl_day_of_week', 'fl_week_of_month', \n",
    "                         'mkt_op_combo', 'fl_type','m_hist_dep_delay', \n",
    "                         'med_hist_dep_delay', 'm_hist_arr_delay','med_hist_arr_delay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval.to_csv(\"X_eval_reg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running our Model\n",
    "now we will run our trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost \n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issues in XGBoost with models between versions \n",
    "# evaluation data ran on original output of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[23:47:42] /Users/travis/build/dmlc/xgboost/src/learner.cc:891: Check failed: header == serialisation_header_: \n\n  If you are loading a serialized model (like pickle in Python) generated by older\n  XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version.  There's a simple script for helping\n  the process. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for reference to the script, and more details about differences between saving model and\n  serializing.\n\n\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000115ceba60 dmlc::LogMessageFatal::~LogMessageFatal() + 112\n  [bt] (1) 2   libxgboost.dylib                    0x0000000115d99972 xgboost::LearnerIO::Load(dmlc::Stream*) + 2274\n  [bt] (2) 3   libxgboost.dylib                    0x0000000115ce6988 XGBoosterUnserializeFromBuffer + 168\n  [bt] (3) 4   libffi.7.dylib                      0x0000000103895ead ffi_call_unix64 + 85\n  [bt] (4) 5   ???                                 0x00007ffeee551d30 0x0 + 140732896976176\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-5cdcbb8dcdb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load in the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Revamp/Regression.sav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             _check_call(\n\u001b[0m\u001b[1;32m   1004\u001b[0m                 _LIB.XGBoosterUnserializeFromBuffer(handle, ptr, length))\n\u001b[1;32m   1005\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'handle'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \"\"\"\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [23:47:42] /Users/travis/build/dmlc/xgboost/src/learner.cc:891: Check failed: header == serialisation_header_: \n\n  If you are loading a serialized model (like pickle in Python) generated by older\n  XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version.  There's a simple script for helping\n  the process. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for reference to the script, and more details about differences between saving model and\n  serializing.\n\n\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000115ceba60 dmlc::LogMessageFatal::~LogMessageFatal() + 112\n  [bt] (1) 2   libxgboost.dylib                    0x0000000115d99972 xgboost::LearnerIO::Load(dmlc::Stream*) + 2274\n  [bt] (2) 3   libxgboost.dylib                    0x0000000115ce6988 XGBoosterUnserializeFromBuffer + 168\n  [bt] (3) 4   libffi.7.dylib                      0x0000000103895ead ffi_call_unix64 + 85\n  [bt] (4) 5   ???                                 0x00007ffeee551d30 0x0 + 140732896976176\n\n"
     ]
    }
   ],
   "source": [
    "#load in the model\n",
    "loaded_model = pickle.load(open('Revamp/Regression.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepping the Submission File\n",
    "we will merge our y_pred with our df_submission for the output csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('Revamp/y_eva.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y[\"0\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.3211  , 32.39949 , 53.694984, ...,  9.233986, 16.441471,\n",
       "       29.089636])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = pd.DataFrame(data = y_pred,\n",
    "                    columns = [\"predicted_delay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = df_submission.merge(y_col, right_index = True, left_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fl_date</th>\n",
       "      <th>mkt_carrier</th>\n",
       "      <th>mkt_carrier_fl_num</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>predicted_delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>WN</td>\n",
       "      <td>5888</td>\n",
       "      <td>ONT</td>\n",
       "      <td>SFO</td>\n",
       "      <td>35.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>WN</td>\n",
       "      <td>6276</td>\n",
       "      <td>ONT</td>\n",
       "      <td>SFO</td>\n",
       "      <td>32.399490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>WN</td>\n",
       "      <td>4598</td>\n",
       "      <td>ONT</td>\n",
       "      <td>SJC</td>\n",
       "      <td>53.694984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>WN</td>\n",
       "      <td>4761</td>\n",
       "      <td>ONT</td>\n",
       "      <td>SJC</td>\n",
       "      <td>44.690716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>WN</td>\n",
       "      <td>5162</td>\n",
       "      <td>ONT</td>\n",
       "      <td>SJC</td>\n",
       "      <td>37.635307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fl_date mkt_carrier  mkt_carrier_fl_num origin dest  predicted_delay\n",
       "0  2020-01-01          WN                5888    ONT  SFO        35.321100\n",
       "1  2020-01-01          WN                6276    ONT  SFO        32.399490\n",
       "2  2020-01-01          WN                4598    ONT  SJC        53.694984\n",
       "3  2020-01-01          WN                4761    ONT  SJC        44.690716\n",
       "4  2020-01-01          WN                5162    ONT  SJC        37.635307"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"regression_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
