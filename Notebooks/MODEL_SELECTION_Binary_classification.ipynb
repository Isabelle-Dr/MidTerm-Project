{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Binary classification\n",
    "**Goal:** The goal is to predict if the flight will be cancelled  \n",
    "\n",
    "**Target variables:** \n",
    "- `CANCELLED` \n",
    "\n",
    "**Notes**:  \n",
    "\n",
    "2% of flights are cancelled. it's a highly imbalanced dataset. We will want to predict probabilities with custom threshold.  \n",
    "[Article on framework for highly imbalanced datasets](https://machinelearningmastery.com/framework-for-imbalanced-classification-projects/)\n",
    "\n",
    "**Steps**:\n",
    "1. Pick evaluation metrics\n",
    "2. Spot check different algos\n",
    "3. Tune hyperparameters using grid search on 2-3 selected algos\n",
    "4. Compare them based on the selected metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Pick evaluation metrics\n",
    "**Brier score** (because we will be using probabilities in some models)  \n",
    "**Precision-Recall AUC** (because positive class is more important)  \n",
    "**F1 score**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Spot check different algos (using K-Folds on sample data)\n",
    "\n",
    "### Step 2.1 Spot check regular algorythms\n",
    "\n",
    "- Naive algorythm (used as a baseline model):  \n",
    "Predict the majority class \n",
    "\n",
    "- Linear algorythms:  \n",
    "Logistic Regression  \n",
    "LDA  \n",
    "- Non linear algos:  \n",
    "k-Nearest Neighbors  \n",
    "\n",
    "- Ensemble algorythms:  \n",
    "Random forest  \n",
    "Stochastic Gradient Boosting   \n",
    "XGBoost  \n",
    " *Optional: custom ensemble*\n",
    "\n",
    "- Other\n",
    "One-Class Support Vector Machines (usually used for outlier detection but can be used to do classification)\n",
    "\n",
    "### Step 2.2 Spot check imbalanced classification algos. Use those techniques on the ML algos to see if it improves performance\n",
    "\n",
    "- Data sampling  \n",
    "Oversampler and Undersampler method: SMOTE combined with RandomUnderSampler on the training datasets in this step [Article on how to perform SMOTE](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)\n",
    "\n",
    "- Probability tuning (custom threshold) on the algos that can give probabilities as outputs  \n",
    "Logistic Regression \n",
    "LDA \n",
    "\n",
    "- Optional: Try a [calibration algo on the predicted prbobabilities](https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn import cross_validation, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('db_binary_sample.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000000 entries, 6552218 to 9619844\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   branded_code_share  1000000 non-null  int64  \n",
      " 1   crs_dep_time        1000000 non-null  int64  \n",
      " 2   crs_arr_time        1000000 non-null  int64  \n",
      " 3   cancelled           1000000 non-null  float64\n",
      " 4   crs_elapsed_time    1000000 non-null  float64\n",
      " 5   air_time            1000000 non-null  float64\n",
      " 6   distance            1000000 non-null  float64\n",
      " 7   fl_month            1000000 non-null  int64  \n",
      " 8   fl_day_of_week      1000000 non-null  int64  \n",
      " 9   fl_type             1000000 non-null  int64  \n",
      " 10  state_travel_type   1000000 non-null  int64  \n",
      " 11  origin_cat          1000000 non-null  int64  \n",
      " 12  dest_cat            1000000 non-null  int64  \n",
      " 13  mkt_op_combo_cat    1000000 non-null  int64  \n",
      "dtypes: float64(4), int64(10)\n",
      "memory usage: 114.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0.0: 982771 values\n",
      "Class 1.0: 17229 values\n",
      "Sample imbalance: 1.7229 %\n"
     ]
    }
   ],
   "source": [
    "# get class representations\n",
    "\n",
    "print(f'Class {data.cancelled.value_counts().index[0]}: {y.value_counts().values[0]} values')\n",
    "print(f'Class {data.cancelled.value_counts().index[1]}: {y.value_counts().values[1]} values')\n",
    "print(f'Sample imbalance: {data.cancelled.value_counts().values[1]/len(y)*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- Trying things -----\n",
    "### Try different splitting methods\n",
    "Trying with `KFold` and `StratifiedShuffleSplit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [100000 100001 100002 ... 999997 999998 999999] TEST: [    0     1     2 ... 99997 99998 99999]\n",
      "TRAIN: [     0      1      2 ... 999997 999998 999999] TEST: [100000 100001 100002 ... 199997 199998 199999]\n",
      "TRAIN: [     0      1      2 ... 999997 999998 999999] TEST: [200000 200001 200002 ... 299997 299998 299999]\n",
      "TRAIN: [     0      1      2 ... 999997 999998 999999] TEST: [300000 300001 300002 ... 399997 399998 399999]\n",
      "TRAIN: [     0      1      2 ... 999997 999998 999999] TEST: [400000 400001 400002 ... 499997 499998 499999]\n",
      "TRAIN: [     0      1      2 ... 999997 999998 999999] TEST: [500000 500001 500002 ... 599997 599998 599999]\n",
      "TRAIN: [     0      1      2 ... 999997 999998 999999] TEST: [600000 600001 600002 ... 699997 699998 699999]\n",
      "TRAIN: [     0      1      2 ... 999997 999998 999999] TEST: [700000 700001 700002 ... 799997 799998 799999]\n",
      "TRAIN: [     0      1      2 ... 999997 999998 999999] TEST: [800000 800001 800002 ... 899997 899998 899999]\n",
      "TRAIN: [     0      1      2 ... 899997 899998 899999] TEST: [900000 900001 900002 ... 999997 999998 999999]\n"
     ]
    }
   ],
   "source": [
    "# try using K-Fold\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X_scaled, y)\n",
    "KFold()\n",
    "\n",
    "for train_index, test_index in kf.split(X_scaled,y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [281750 943380 795722 ... 660010 884461 260970] TEST: [276253 447818 356965 ... 975127  45228 627114]\n",
      "TRAIN: [591858 801835 762344 ... 286665 458622 994887] TEST: [996020 796429 511687 ... 620788 815472 405986]\n",
      "TRAIN: [576527 806177 930996 ...  90238 846362 903068] TEST: [167251 488581 985668 ... 560869 182965 112546]\n",
      "TRAIN: [ 11082  66792  97360 ...  85078 995251 140024] TEST: [999600 458983 700206 ... 939295 780817 200095]\n",
      "TRAIN: [459230 383268 532306 ...   5436 749097 297154] TEST: [232992 863587   5703 ...  11039 215364 768469]\n",
      "TRAIN: [362763 729592 322726 ...  39634 497503 250751] TEST: [984895 653479 547897 ... 736018 312592 814918]\n",
      "TRAIN: [ 66554 528276 832008 ... 295657 818478 957125] TEST: [152964 610160 944232 ... 305574 897138 353157]\n",
      "TRAIN: [581090 849604   6935 ... 851652 875719 431848] TEST: [571587 352508 237495 ... 528434 892110 135950]\n",
      "TRAIN: [ 81802  24268  53156 ... 323598 371099 539020] TEST: [128670 551496 672841 ...  33865 102943 810189]\n",
      "TRAIN: [201134 864831 998253 ... 653018 952151 155077] TEST: [962644 740510 741916 ... 490095 924245 733363]\n"
     ]
    }
   ],
   "source": [
    "# try using StratifiesShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.75, random_state=0)\n",
    "sss.get_n_splits(X_scaled, y)\n",
    "\n",
    "for train_index, test_index in sss.split(X_scaled, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try syntax for training and predicting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import selected metrics\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# import model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get train/test data\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X_scaled, y, train_size=0.80, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# create placeolder\n",
    "model = LogisticRegression(random_state=0)\n",
    "\n",
    "# fit the data\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# predict\n",
    "y_pred = model.predict(X_test)\n",
    "brier_score = brier_score_loss(y_test, y_pred)\n",
    "print(brier_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying using cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_brier_score\n",
      "Mean:\t-8.30031802523422e-06\n",
      "Std.:\t0.0\n",
      "roc_auc\n",
      "Mean:\t0.999999682279595\n",
      "Std.:\t0.0\n",
      "f1\n",
      "Mean:\t0.9999709850573044\n",
      "Std.:\t0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# placeholder\n",
    "model = LogisticRegression(random_state=0)\n",
    "\n",
    "# get the data, scale X\n",
    "y = data.cancelled\n",
    "X = data.drop('cancelled', axis = 1)\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# get the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# get the scorings\n",
    "scoring = ('neg_brier_score', 'roc_auc', 'f1')\n",
    "\n",
    "# get scores\n",
    "cv_results = cross_validate(model, X_scaled, y, cv = n_folds, scoring = scoring)\n",
    "print(f\"neg_brier_score\\nMean:\\t{cv_results['test_neg_brier_score'].mean()}\\nStd.:\\t{cv_results['test_neg_brier_score'].mean().std()}\")\n",
    "print(f\"roc_auc\\nMean:\\t{cv_results['test_roc_auc'].mean()}\\nStd.:\\t{cv_results['test_roc_auc'].mean().std()}\")\n",
    "print(f\"f1\\nMean:\\t{cv_results['test_f1'].mean()}\\nStd.:\\t{cv_results['test_f1'].mean().std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ End of trying things ----- back at checking algos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1 Spot check regular algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data and parameters\n",
    "\n",
    "# get the data, scale X\n",
    "y = data.cancelled\n",
    "X = data.drop('cancelled', axis = 1)\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# get the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# get the scorings\n",
    "scoring = ('neg_brier_score', 'roc_auc', 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive algorythms: predicting class 0 for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_brier_score\n",
      "Mean:\t-0.017228999999999998\n",
      "Std.:\t0.0\n",
      "roc_auc\n",
      "Mean:\t0.5\n",
      "Std.:\t0.0\n",
      "f1\n",
      "Mean:\t0.0\n",
      "Std.:\t0.0\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# placeholder\n",
    "model = DummyClassifier(strategy = \"most_frequent\")\n",
    "\n",
    "# get scores\n",
    "cv_results = cross_validate(model, X_scaled, y, cv = n_folds, scoring = scoring)\n",
    "print(f\"neg_brier_score\\nMean:\\t{cv_results['test_neg_brier_score'].mean()}\\nStd.:\\t{cv_results['test_neg_brier_score'].mean().std()}\")\n",
    "print(f\"roc_auc\\nMean:\\t{cv_results['test_roc_auc'].mean()}\\nStd.:\\t{cv_results['test_roc_auc'].mean().std()}\")\n",
    "print(f\"f1\\nMean:\\t{cv_results['test_f1'].mean()}\\nStd.:\\t{cv_results['test_f1'].mean().std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_brier_score\n",
      "Mean:\t-8.30031802523422e-06\n",
      "Std.:\t0.0\n",
      "roc_auc\n",
      "Mean:\t0.999999682279595\n",
      "Std.:\t0.0\n",
      "f1\n",
      "Mean:\t0.9999709850573044\n",
      "Std.:\t0.0\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# placeholder\n",
    "model = LogisticRegression()\n",
    "\n",
    "# get scores\n",
    "cv_results = cross_validate(model, X_scaled, y, cv = n_folds, scoring = scoring)\n",
    "print(f\"neg_brier_score\\nMean:\\t{cv_results['test_neg_brier_score'].mean()}\\nStd.:\\t{cv_results['test_neg_brier_score'].mean().std()}\")\n",
    "print(f\"roc_auc\\nMean:\\t{cv_results['test_roc_auc'].mean()}\\nStd.:\\t{cv_results['test_roc_auc'].mean().std()}\")\n",
    "print(f\"f1\\nMean:\\t{cv_results['test_f1'].mean()}\\nStd.:\\t{cv_results['test_f1'].mean().std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_brier_score\n",
      "Mean:\t-0.003590931248893237\n",
      "Std.:\t0.0\n",
      "roc_auc\n",
      "Mean:\t0.9998115128775324\n",
      "Std.:\t0.0\n",
      "f1\n",
      "Mean:\t0.8720059949150183\n",
      "Std.:\t0.0\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# placeholder\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# get scores\n",
    "cv_results = cross_validate(model, X_scaled, y, cv = n_folds, scoring = scoring)\n",
    "print(f\"neg_brier_score\\nMean:\\t{cv_results['test_neg_brier_score'].mean()}\\nStd.:\\t{cv_results['test_neg_brier_score'].mean().std()}\")\n",
    "print(f\"roc_auc\\nMean:\\t{cv_results['test_roc_auc'].mean()}\\nStd.:\\t{cv_results['test_roc_auc'].mean().std()}\")\n",
    "print(f\"f1\\nMean:\\t{cv_results['test_f1'].mean()}\\nStd.:\\t{cv_results['test_f1'].mean().std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_brier_score\n",
      "Mean:\t-7.579000000000008e-06\n",
      "Std.:\t0.0\n",
      "roc_auc\n",
      "Mean:\t1.0\n",
      "Std.:\t0.0\n",
      "f1\n",
      "Mean:\t1.0\n",
      "Std.:\t0.0\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# placeholder\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# get scores\n",
    "cv_results = cross_validate(model, X_scaled, y, cv = n_folds, scoring = scoring)\n",
    "print(f\"neg_brier_score\\nMean:\\t{cv_results['test_neg_brier_score'].mean()}\\nStd.:\\t{cv_results['test_neg_brier_score'].mean().std()}\")\n",
    "print(f\"roc_auc\\nMean:\\t{cv_results['test_roc_auc'].mean()}\\nStd.:\\t{cv_results['test_roc_auc'].mean().std()}\")\n",
    "print(f\"f1\\nMean:\\t{cv_results['test_f1'].mean()}\\nStd.:\\t{cv_results['test_f1'].mean().std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_brier_score\n",
      "Mean:\t-7.587800000000011e-06\n",
      "Std.:\t0.0\n",
      "roc_auc\n",
      "Mean:\t1.0\n",
      "Std.:\t0.0\n",
      "f1\n",
      "Mean:\t1.0\n",
      "Std.:\t0.0\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# placeholder\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# get scores\n",
    "cv_results = cross_validate(model, X_scaled, y, cv = n_folds, scoring = scoring)\n",
    "print(f\"neg_brier_score\\nMean:\\t{cv_results['test_neg_brier_score'].mean()}\\nStd.:\\t{cv_results['test_neg_brier_score'].mean().std()}\")\n",
    "print(f\"roc_auc\\nMean:\\t{cv_results['test_roc_auc'].mean()}\\nStd.:\\t{cv_results['test_roc_auc'].mean().std()}\")\n",
    "print(f\"f1\\nMean:\\t{cv_results['test_f1'].mean()}\\nStd.:\\t{cv_results['test_f1'].mean().std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Class Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# placeholder\n",
    "model = OneClassSVM()\n",
    "\n",
    "# get scores\n",
    "cv_results = cross_validate(model, X_scaled, y, cv = n_folds, scoring = scoring)\n",
    "print(f\"neg_brier_score\\nMean:\\t{cv_results['test_neg_brier_score'].mean()}\\nStd.:\\t{cv_results['test_neg_brier_score'].mean().std()}\")\n",
    "print(f\"roc_auc\\nMean:\\t{cv_results['test_roc_auc'].mean()}\\nStd.:\\t{cv_results['test_roc_auc'].mean().std()}\")\n",
    "print(f\"f1\\nMean:\\t{cv_results['test_f1'].mean()}\\nStd.:\\t{cv_results['test_f1'].mean().std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "import xgboost as xgb\n",
    "\n",
    "# placeholder\n",
    "model = xgb.XGBClassifier(scale_pos_weight=100) # parameter set because the dataset is imbalanced\n",
    "\n",
    "# get scores\n",
    "cv_results = cross_validate(model, X_scaled, y, cv = n_folds, scoring = 'roc_auc')\n",
    "# print(f\"neg_brier_score\\nMean:\\t{cv_results['test_neg_brier_score'].mean()}\\nStd.:\\t{cv_results['test_neg_brier_score'].mean().std()}\")\n",
    "# print(f\"roc_auc\\nMean:\\t{cv_results['test_roc_auc'].mean()}\\nStd.:\\t{cv_results['test_roc_auc'].mean().std()}\")\n",
    "# print(f\"f1\\nMean:\\t{cv_results['test_f1'].mean()}\\nStd.:\\t{cv_results['test_f1'].mean().std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([31.20423722, 31.55741978, 33.59384584, 34.10717416, 34.16739893]),\n",
       " 'score_time': array([0.19097233, 0.20549273, 0.19444442, 0.18151617, 0.16628432]),\n",
       " 'test_score': array([1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Hyperparameter tuning\n",
    "Use Grid Search to tune the selected algo (on the whole dataframe?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
